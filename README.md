# Attention is all you need (to implement)
A deacade into the attention craze, this repository casts the era of deep learning step by step from the [paper](https://arxiv.org/abs/1706.03762) that started it all for LLMs. Some of the content presented pre-dates Transformers while other work is fairly recent.


1. Transformer Architecture
2. Neural Translation
